{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-joshua",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindmichelle/mask_rcnn_istar/blob/master/epoch%205%20head%20%2C%205%20all%20with%20LR%2010-e3%2C%2010%20-e4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqI_Mh7VDk4q",
        "colab_type": "code",
        "outputId": "25281f6f-94e3-422d-a46e-7ea3f837f5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 116.76 MiB | 38.83 MiB/s, done.\n",
            "Resolving deltas: 100% (567/567), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1vwj_rxHJmM",
        "colab_type": "code",
        "outputId": "4dc92018-7dde-452b-8e17-3c88b2eca495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIBap-vRkHYU",
        "colab_type": "code",
        "outputId": "c6176c19-53bf-4e0f-a9d3-4cfd77b82fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd\n",
        "%cd ../content/drive/\"My Drive\"/\"dataset zip\"/\"all\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/drive/My Drive/dataset zip/all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uT6k7bTkQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile('dataset_05_10.zip', 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkbPu-_xS2ra",
        "colab_type": "text"
      },
      "source": [
        "## Install Keras 2.1.6\n",
        "To allow multi GPU Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JNTDl45MRQg",
        "colab_type": "code",
        "outputId": "5387ba12-3b19-4098-d1a8-42aa7f6231fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "%pip install keras==2.1.6"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99qXXLHFxS9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a601124-359c-45ad-f816-c87d72fa110c"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkAOqTLHDzxF",
        "colab_type": "code",
        "outputId": "63eab84d-5d06-4c2a-8ee3-662b1448aecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd\n",
        "%cd ../content/Mask_RCNN/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/Mask_RCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl5O4MmvTEih",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXhawPOUD4_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de24527b-94cd-4bdc-87c8-1c2729b7b581"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.draw\n",
        "import json\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading pretrained model to /content/mask_rcnn_coco.h5 ...\n",
            "... done downloading pretrained model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d32l0FKP2XZ",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eLL245PQRCS",
        "colab_type": "code",
        "outputId": "99023343-131a-4062-8049-150232dd99f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "class IstarConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy  dataset.\n",
        "    Derives from the base Config class and overrides some values.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    BACKBONE = \"resnet50\"\n",
        "    LEARNING_RATE = 0.001\n",
        "    GPU_COUNT = 1\n",
        "\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 5  # Background + 5 istar elements + 2 istar actor types\n",
        "\n",
        "    STEPS_PER_EPOCH = 171\n",
        "    BATCH_SIZE=1\n",
        "\n",
        "    # NOTE : hyperparams check\n",
        "    # DETECTION_MIN_CONFIDENCE = 0.01\n",
        "    # DETECTION_NMS_THRESHOLD = 0.99\n",
        "\n",
        "\n",
        "config = IstarConfig()\n",
        "config.display()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet50\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                18\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           shapes\n",
            "NUM_CLASSES                    6\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                171\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNK2aW07ZA-L",
        "colab_type": "text"
      },
      "source": [
        "## Notebook Preferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Jl-78WY_7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4-54UGPxrM",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Create a synthetic dataset\n",
        "\n",
        "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
        "\n",
        "* load_image()\n",
        "* load_mask()\n",
        "* image_reference()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVtsLra9QyHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.transform import rotate\n",
        "\n",
        "class IstarDataset(utils.Dataset):\n",
        "\n",
        "    def load_istar(self, dataset_dir, subset):\n",
        "        \"\"\"Load a subset of the Balloon dataset.\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: Subset to load: train or val\n",
        "        \"\"\"\n",
        "        self.add_class(\"shapes\", 1, \"Actor\")\n",
        "        self.add_class(\"shapes\", 2, \"Goal\")\n",
        "        self.add_class(\"shapes\", 3, \"Quality\")\n",
        "        self.add_class(\"shapes\", 4, \"Task\")\n",
        "        self.add_class(\"shapes\", 5, \"Resource\")\n",
        "        \n",
        "\n",
        "        # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"val\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "        # Load annotations\n",
        "        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n",
        "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
        "        #   'regions': {\n",
        "        #       '0': {\n",
        "        #           'region_attributes': {},\n",
        "        #           'shape_attributes': {\n",
        "        #               'all_points_x': [...],\n",
        "        #               'all_points_y': [...],\n",
        "        #               'name': 'polygon'}},\n",
        "        #       ... more regions ...\n",
        "        #   },\n",
        "        #   'size': 100202\n",
        "        # }\n",
        "        # We mostly care about the x and y coordinates of each region\n",
        "        # Note: In VIA 2.0, regions was changed from a dict to a list.\n",
        "        annotationsDict = json.load(open(os.path.join(dataset_dir, \"annotations.json\")))\n",
        "        annotationsArr = list(annotationsDict.values())  # don't need the dict keys\n",
        "\n",
        "        \n",
        "        # The VIA tool saves images in the JSON even if they don't have any\n",
        "        # annotations. Skip unannotated images.\n",
        "        annotations = [a for a in annotationsArr if a['regions']]\n",
        "\n",
        "        # Add images\n",
        "        for a in annotations:\n",
        "            # Get the x, y coordinaets of points of the polygons that make up\n",
        "            # the outline of each object instance. These are stores in the\n",
        "            # shape_attributes (see json format above)\n",
        "            # The if condition is needed to support VIA versions 1.x and 2.x.\n",
        "            # print('filename ', a['filename'])\n",
        "            polygons = [r['shape_attributes'] for r in a['regions'].values()]\n",
        "            objects = [s['region_attributes'] for s in a['regions'].values()]\n",
        "\n",
        "            ## NOTE : this contains label of the objects\n",
        "            class_ids_name = [int(n['element_name']) for n in objects]\n",
        "            # class_ids_type = [int(n['element_type']) for n in objects]\n",
        "\n",
        "\t\t\t# load_mask() needs the image size to convert polygons to masks.\n",
        "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
        "            # the image. This is only managable since the dataset is tiny.\n",
        "            # print(\"multi_numbers=\", multi_numbers)\n",
        "            # num_ids = [n for n in multi_numbers['number'].values()]\n",
        "            # for n in multi_numbers:\n",
        "            \n",
        "            image_path = os.path.join(dataset_dir, a['filename'])\n",
        "            image = skimage.io.imread(image_path, plugin='pil')\n",
        "            isLandscape= a['file_attributes']['landscape']\n",
        "            height, width = image.shape[:2]\n",
        "\n",
        "            if isLandscape :\n",
        "              if a['filename'] == '20200218_165243_noise_test.jpg' : \n",
        "                print(a['filename'], height, width)\n",
        "\n",
        "              if width < height :\n",
        "                if a['filename'] == '20200218_165243_noise_test.jpg' : \n",
        "                  print('should be rotate')\n",
        "                image = rotate(image, 90, resize=True)\n",
        "                height, width = image.shape[:2]\n",
        "                \n",
        "                if a['filename'] == '20200218_165243_noise_test.jpg' : \n",
        "                  print('after rotate (h,w) : ', height, width)\n",
        "\n",
        "                # print(\"should be rotate into landscape\")\n",
        "            \n",
        "            else : #if portrait\n",
        "              if height < width : \n",
        "                image = rotate(image, 90, resize=True)\n",
        "                height, width = image.shape[:2]\n",
        "                # print(\"should be rotate into portrait\")\n",
        "\n",
        "            self.add_image(\n",
        "                \"shapes\",\n",
        "                image_id=a['filename'],  # use file name as a unique image id\n",
        "                path=image_path,\n",
        "                width=width, height=height,\n",
        "                polygons=polygons,\n",
        "\t\t\t\tclass_ids=class_ids_name)\n",
        "\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a balloon dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "\n",
        "        # print(\"IMAGE INFO :\", image_info)\n",
        "        if image_info[\"source\"] != \"shapes\" :\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "        class_ids = image_info['class_ids']\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        info = self.image_info[image_id]\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.uint8)\n",
        "        for i, p in enumerate(info[\"polygons\"]):\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            if p['name'] == 'polyline':\n",
        "              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            elif p['name'] == 'circle':\n",
        "              rr, cc = skimage.draw.circle(p['cy'], p['cx'], p['r'])\n",
        "            # rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            mask[rr, cc, i] = 1\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID only, we return an array of 1s\n",
        "        #class_ids=np.array([self.class_names.index(shapes[0])])\n",
        "        # print(\"info['class_ids']=\", info['class_ids'])\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        return mask, class_ids\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQnoOIVCPoB6",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Dataset for Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW_6OeHSRosx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training dataset\n",
        "dataset_train = IstarDataset()\n",
        "dataset_train.load_istar(\"../dataset_05_10/\", \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = IstarDataset()\n",
        "dataset_val.load_istar(\"../dataset_05_10/\", \"val\")\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fFjDE4y9lC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "\n",
        "augmentation = iaa.Sometimes(2/3,iaa.OneOf(\n",
        "                                            [\n",
        "                                            iaa.Grayscale(alpha=(0.0, 1.0)),\n",
        "                                            iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
        "                                             ]\n",
        "                                        )\n",
        "                                   )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn7XcwwdPYA8",
        "colab_type": "text"
      },
      "source": [
        "## Create Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sJGBvMqKYHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "51073fef-e1c0-4060-ff52-ca3deccee565"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7O4ePWRTzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "1305eaea-ad87-4e82-f6a2-1f82f7c2fe4e"
      },
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"coco\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "      # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:180: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1-ApVhWRG-C",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc6Hw37kK1GD",
        "colab_type": "code",
        "outputId": "9d5f80a2-0711-4328-c459-80a5467a6a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "# TODO :  find out difference between train only on heads layer and all layers\n",
        "model.train(dataset_train, dataset_val, learning_rate=config.LEARNING_RATE, augmentation=augmentation,\n",
        "            epochs=5, \n",
        "            layers='heads')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/logs/shapes20200611T1952/mask_rcnn_shapes_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/5\n",
            "116/171 [===================>..........] - ETA: 39s - loss: 1.6635 - rpn_class_loss: 0.0674 - rpn_bbox_loss: 0.2857 - mrcnn_class_loss: 0.6036 - mrcnn_bbox_loss: 0.4219 - mrcnn_mask_loss: 0.2849"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzhAHNOsRAw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Fine tune all layers\n",
        "# # Passing layers=\"all\" trains all layers. You can also \n",
        "# # pass a regular expression to select which layers to\n",
        "# # train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=10, augmentation=augmentation,\n",
        "            layers=\"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Qo-HZUe8wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cp -r ../logs/ ../drive/\"My Drive\"/logs_5/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH0R13PJftwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cp -r ../logs/ ../drive/\"My Drive\"/logs_3/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svR4niumRkVi",
        "colab_type": "text"
      },
      "source": [
        "## Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eJvWgWkRhdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceConfig(IstarConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    # DETECTION_NMS_THRESHOLD = 0.01\n",
        "    # DETECTION_MIN_CONFIDENCE = 0.5\n",
        "\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "# model_path = '../logs/shapes20200414T0653/mask_rcnn_shapes_0030.h5'\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqV0cA9RySA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "print(\"image_id\", image_id)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "log(\"image_id\", image_id)\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9rvsxgwR27K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Nfy_KoR7MC",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axlAvVYFuL1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = dataset_val.image_ids\n",
        "\n",
        "# image_ids = [12]\n",
        "APs = []\n",
        "APs_range = []\n",
        "\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    \n",
        "    # For debugging\n",
        "    # print('image_id',image_id)\n",
        "    # info = dataset_val.image_info[image_id]\n",
        "    # print('image info ', info)\n",
        "    # print('rois', r['rois'].shape)\n",
        "    # print('gt_bbox', gt_bbox.shape)\n",
        "\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "\n",
        "    new_ap = utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
        "                                    r[\"rois\"], r[\"class_ids\"], r[\"scores\"], \n",
        "                                    r['masks'], verbose=0)\n",
        "    # print(\"ap range,\", new_ap)\n",
        "\n",
        "    APs.append(AP)\n",
        "    APs_range.append(new_ap)\n",
        "    # ax = get_ax()\n",
        "    # visualize.display_differences(image, gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'],r['scores'],r['masks'],\n",
        "    #                           dataset_val.class_names,ax=ax)\n",
        "\n",
        "\n",
        "print(\"mAP50: \", np.mean(APs))\n",
        "print(\"mAP range: \", np.mean(APs_range))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV0gvIs9CEAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd\n",
        "# %cd ../content/drive/\"My Drive\"/\n",
        "# %rm -rf ./logs/\n",
        "# %cd\n",
        "# %cd ../content/\n",
        "# %cp -r ./logs/\"gleam only\" ./drive/\"My Drive\"/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}